%
% 4-hessesche.tex
%
% (c) 2023 Prof Dr Andreas Müller
%
\section{Hessesche Matrix
\label{buch:fuvar:section:hessesche}}
\kopfrechts{Hessesche Matrix}
Das Verschwinden der Ableitung einer differenzierbaren Funktion
$f\colon\mathbb{R}\to\mathbb{R}$ ist eine notwendige Bedingung
für ein Extremum, sie ist aber nicht hinreichend.
Im Grundlagenunterricht in der Analysis lernt man, dass man mit
Hilfe der zweiten Ableitung zu einem hinreichenden Kriterium
kommen kann.
Ist die zweite Ableitung $f''(x_0)$ an einer Stelle $x_0$ grösser
als $0$, dann liegt ein Minimum vor, falls sie negativ ist, liegt
ein Maximum vor.
In diesem Abschnitt soll gezeigt werden, wie dieses Kriterium auf
Funktionen mehrere Variablen verallemeinert werden kann.

%
% Die zweite Ableitung
%
\subsection{Die zweite Ableitung}
In diesem Abschnitt ist $f\colon\mathbb{R}^n\to\mathbb{R}$
eine zweimal stetig differenzierbare Funktionen, deren Gradient
an der Stelle $x_0$ verschwindet.
Durch eine Translation $x\mapsto x-x_0$ kann man immer erreichen,
dass $x_0=0$ der Nullpunkt des Koordinatensystems ist.
In der folgenden Diskussion wird diese Konvention wo zweckmässig
stillschweigend verwendet.

%
% Die Hessesche Matrix
%
\subsubsection{Die Hessesche Matrix}
Damit der Punkt $x_0$ ein Minimum ist, muss jede auf die Richtung
$v\in\mathbb{R}^n$ eingeschränkte Funktion eine positive zweite
Ableitung haben.
Es muss also für alle $v\in\mathbb{R}^n$
\[
\frac{d^2}{dt^2} f(x_0+vt)\bigg|_{t=0}
> 0
\]
sein.
Wenn das umgekehrte Zeichen gilt, dann liegt ein Maximum vor.
Ausgeschrieben in Komponenten gilt
\[
\frac{d^2}{dt^2} f(x_0+vt)
=
\sum_{i,k=1}^n
\frac{\partial^2 f}{\partial x_i\,\partial x_k}(x_0+vt)
\,
v_iv_k,
\]
wobei die $v_i$ die Komponenten von $v$ sind.
An der Stelle $t=0$ bleibt die Eigenschaft
\[
\frac{d^2}{dt^2}f(x_0+vt)_{t=0}
=
\sum_{i,k=1}^n \frac{\partial^2 f}{\partial x_i\,\partial x_k}(x_0) \,v_iv_k
>
0.
\]
Die Summe kann mit Hilfe der Matrix $H$ mit den Einträgen
\[
h_{ik}
=
\frac{\partial^2 f}{\partial x_i\,\partial x_k}(x_0)
\]
kompakter als
\[
\frac{d^2}{dt^2}f(x_0+vt)_{t=0}
=
\sum_{i,k=1}^n \frac{\partial^2 f}{\partial x_i\,\partial x_k}(x_0) \,v_iv_k
=
v^tHv
>
0
\]
geschrieben werden.
Ein hinreichendes Kriterium für ein Minimum wird also zu einer
Eigenschaft der Matrix $H$.

\begin{definition}[Hessesche Matrix]
Ist $f\colon\mathbb{R}^n\to\mathbb{R}$ eine zweimal stetig differenzierbare
Funktion, dann heisst die Matrix
\[
H(x_0)
=
\begin{pmatrix}
h_{11}(x_0)&\dots &h_{1n}(x_0)\\
\vdots&\ddots&\vdots\\
h_{n1}(x_0)&\dots &v_{nn}(x_0)
\end{pmatrix}
\qquad\text{mit}\qquad
h_{ik}(x_0)
=
\frac{\partial^2 f}{\partial x_i\,\partial x_k}(x_0)
\]
die {\em hessesche Matrix}.
\end{definition}

Da die Funktion $f$ zweimal stetig differenzierbar angenommen wurde,
sind die partiellen Ableitungen nach dem Satz von Schwarz
\[
\frac{\partial^2 f}{\partial x_i\,\partial x_k}(x_0)
=
\frac{\partial^2 f}{\partial x_k\,\partial x_i}(x_0)
\]
vertauschbar.
Die hessesche Matrix $H(x_0)$ ist daher immer symmetrisch: $H(x_0)^t=H(x_0)$.

%
% Notwendiges Kriterium für ein Extremum
%
\subsubsection{Notwendiges Kriterium für ein Extremum}
Das Vorzeichen von $v^tH(x_0)v$ entscheidet darüber, ob ein Extremum
vorliegt.
Dies führt auf die folgende Definition.

\begin{definition}
Eine symmetrische Matrix $A$ heisst {\em positiv definit}, wenn 
\index{positiv definit}%
\index{definit, positiv}%
$v^tAv>0$ für alle Vektoren $v\in\mathbb{R}^n\setminus\{0\}$ ist.
Sie heisst {\em positiv semidefinit}, wenn $v^tAv\ge 0$ für alle
Vektoren
\index{positiv semidefinit}%
\index{semidefinit, positiv}%
$v\in\mathbb{R}^n\setminus\{0\}$ ist.
\end{definition}

\begin{satz}[Hinreichendes Kriterium für Extremum]
\label{buch:fuvar:hessesche:satz:kriterium}
Wenn die hessesche Matrix einer zweimal stetig differenzierbaren
Funktion $f\colon\mathbb{R}^n\to\mathbb{R}$ mit $\grad f(x_0)=0$ 
an der Stelle $x_0$ positiv definit ist, dann hat $f$ an dieser
Stelle ein Minimum.
Wenn die hessesche Matrix negativ definit ist, dann liegt ein Maximum
vor.
\end{satz}

Im Falle einer Funktion einer Variablen lässt sich für $f'(x_0)=0$
und $f''(x_0)=0$ keine Aussage darüber machen lässt, ob ein Maximum 
oder Minimum vorliegt.
Bei einer Funktion mehrerer Variablen verunmöglicht bereits Semidefinitheit
der hesseschen Matrix, dass an der Stelle $x_0$ ein Extremum vorliegt.
Zum Beispiel hat die Funktion $f(x,y)=x^2+y^3$ die hessesche Matrix
\[
H(0)
=
\begin{pmatrix}
2&0\\
0&0
\end{pmatrix}
\qquad\text{die wegen}\qquad
v^tH(0)v = v_1^2 \ge 0
\]
positiv semidefinit ist.
Trotzdem hat $f$ im Nullpunkt kein Extremum.
Andererseits hat die Funktion $f(x,y)=x^2+y^4$ die gleiche hessesche
Matrix im Nullpunkt, es liegt dort aber ein Minimum vor.

%
% Extremalkriterien
%
\subsubsection{Extremalkriterien}
Das definierende Kriterium der positiven Definitheit, dass $v^tH(x_0)v>0$
für alle Vektoren $v\in\mathbb{R}^n\setminus\{0\}$ sein muss, ist
relativ schwer nachzuprüfen.
Daher sind alternative Kriterien wünschenswert, die leichter zu überprüfen
sind.

\begin{satz}[positive Eigenwerte]
\label{buch:fuvar:hessesche:satz:positiveeigenwerte}
Ist $f\colon\mathbb{R}^n\to\mathbb{R}$ eine zweimal stetig differenzierbare
Funktion mit $\grad f(x_0)=0$, dann liegt an der Stelle $x_0$ ein Minimum
vor, wenn alle Eigenwerte der hesseschen Matrix positiv sind.
\end{satz}

\begin{proof}
Da die hessesche Matrix symmetrisch ist, lässt sie sich orthogonal
diagonalisieren.
Seien $u_i$ orthonormierte Eigenvektoren mit $Hu_i=\lambda_iu_i$ und
den Eigenwerten $\lambda_i>0$.
Jeder Vektor $v$ kann also als Linearkombination
\[
v = \sum_{i=1}^n v_iu_i
\]
der Eigenvektoren $u_i$ geschrieben werden und 
\[
v^tHv
=
\sum_{i,k=1}^n
v_ku_k^t
Hv_iu_i
=
\sum_{i,k=1}^n v_iv_k\lambda_i \underbrace{u_k^tu_i}_{\displaystyle=\delta_{ik}}
=
\sum_{i=1}^{n} v_i^2\lambda_i > 0,
\]
somit ist die hessesche Matrix positiv definit und es liegt ein
Minimum vor.
\end{proof}

Die Anwendung von Satz~\ref{buch:fuvar:hessesche:satz:positiveeigenwerte}
verlangt die Berechnung aller Eigenwerte der hesseschen Matrix, was für
$n>2$ eher beschwerlich ist.
Für eine symmetrische Matrix gibt es aber ein numerisch sehr einfach
nachzuprüfendes Kriterium für positive Definitheit.

\begin{satz}[Cholesky-Zerlegung]
Sei $f\colon\mathbb{R}^n\to\mathbb{R}$ eine zweimal stetig differenzierbare
Funktion mit $\grad f(x_0)=0$.
Falls es eine untere Dreiecksmatrix $L$ mit $\det L\ne 0$ gibt derart,
dass $H(x_0)=LL^t$ ist, dann liegt an der Stelle $x_0$ ein Minimum vor.
\end{satz}

\begin{proof}
Für jeden beliebigen Vektor $v\in\mathbb{R}\setminus\{0\}$ folgt
\[
v^tHv
=
v^tLL^tv
=
(L^tv)^t L^tv
=
|L^tv|^2
\ge
0.
\]
Da die Matrix $L$ ausserdem invertierbar ist, gilt sogar $v^tHv>0$.
Damit folgt, dass die hessesche Matrix positiv definit ist und dass
ein Minimum vorliegt.
\end{proof}

Die Zerlegung $H=LL^t$ heisst auch die {\em Cholesky-Zerlegung}, für die
\label{Cholesky-Zerlegung}%
es einen numerischen Algorithmus gibt, der sehr viel effizienter
durchzuführen ist als die Bestimmung der Eigenwerte
\cite[Abschnitt 12.3]{buch:linalg}.


%
% Die Taylor-Reihe für Funktionen mehrere Variablen
%
\subsection{Extrema und höhere Ableitungen
\label{buch:fuvar:hessesche:subsection:extrema}}
Verschwinden für eine Funktion einer Variablen auch höhere als die erste
Ableitung, zieht man gemeinhin die Taylor-Reihe
\begin{align*}
\mathscr{T_{x_0}}f(x)
&=
f(x_0) + f'(x_0)(x-x_0)
+
\frac{f''(x_0)}{2!}(x-x_0)^2
+
\frac{f'''(x_0)}{3!}(x-x_0)^3
+
\dots
\\
&=
\sum_{k=0}^\infty \frac{f^{(k)}(x_0)}{k!}(x-x_0)^k
\end{align*}
im Punkt $x_0$ hinzu, um zu entscheiden, ob im Punkt $x_0$ ein Extremum
vorliegt.
Ist die erste nicht verschwindende Ableitung von gerade Ordnung $2r$, dann
liegt genau dann ein Maximum vor, wenn $f^{(2r)}(x_0)<0$ ist, andernfalls
liegt ein Minimum vor.
Ist die erste nicht verschwindende Ableitung dagegen von ungerader Ordnung,
liegt ein Sattelpunkt vor, also kein Extremum.
Diese Argumentationsweise lässt sich auf auf Funktionen von mehreren
Variablen übertragen, was in diesem Abschnitt geschehen soll.
Wir nehmen im Folgenden an, dass alle vorkommenden Funktionen konvergente
Taylor-Reihen haben.

%
% Taylor-Reihe
%
\subsubsection{Die Taylor-Reihe für Funktionen zweier Variablen}
Die partiellen Funktionen einer Funktion von zwei Variablen lassen
sich in Taylor-Reihen entwickeln:
\begin{align}
f(x,y)
&=
\mathscr{T}_{x_0} f(x,y)
=
f(x_0,y)
+
\frac{\partial f}{\partial x}(x_0,y) (x-x_0)
+
\frac{1}{2!}
\frac{\partial^2 f}{\partial x^2}(x_0,y) (x-x_0)^2
%+
%\frac{1}{3!}
%\frac{\partial^3 f}{\partial x^3}(x_0,y_0) (x-x_0)^3
+
\dots
\label{buch:fuvar:hessesche:taylor:Tfx}
\\
%f(x,y)
&=
\mathscr{T}_{y_0} f(x,y)
=
f(x,y_0)
+
\frac{\partial f}{\partial y}(x,y_0)(y-y_0)
+
\frac{1}{2!}
\frac{\partial^2 f}{\partial y^2}(x,y_0) (y-y_0)^2
%+
%\frac{1}{3!}
%\frac{\partial^3 f}{\partial y^3}(x_0,y_0) (y-y_0)^3
+
\dots
\label{buch:fuvar:hessesche:taylor:Tfy}
\end{align}
In jedem Term ist der Koeffizient eine Funktion von $y$, die als
Taylor-Reihe entwickelt werden kann.
\begin{equation}
\renewcommand{\arraycolsep}{0pt}
\renewcommand{\arraystretch}{2.2}
\rlap{\raisebox{-1.63cm}{\begin{tikzpicture}[thick]
\draw[color=white] (0,0) rectangle (1,3.4);
\fill[color=darkred!10] (1.27,0) rectangle (4.77,3.4);
\fill[color=blue!10] (4.81,0) rectangle (9.77,3.4);
\fill[color=white] (4.79,2.55) rectangle (5.1,3.41);
\end{tikzpicture}}}
\begin{array}{cccrclcrclcrclc}
f(x,y)
&\;=\;& &f(x_0,y_0)&&
&\mathstrut+\mathstrut
&\displaystyle\frac{\partial f}{\partial x}(x_0,y_0)&\cdot&(x-x_0)
%&+&\displaystyle\frac1{2!}\frac{\partial^2 f}{\partial x^2}(x_0,y_0)(x-x_0)^2
&\mathstrut+\mathstrut
&&&\dots
\\
&
&\mathstrut+\mathstrut
&\displaystyle\frac{\partial f}{\partial y}(x_0,y_0)&\cdot&(y-y_0)
&\mathstrut+\mathstrut
&\displaystyle\frac{\partial^2 f}{\partial y\,\partial x}(x_0,y_0)&\cdot&(y-y_0)(x-x_0)
%&+&\displaystyle\frac{1}{2!}\frac{\partial^3 f}{\partial y\,\partial x}(x_0,y_0)(y-y_0)(x-x_0)
&\mathstrut+\mathstrut
&&&\dots
\\
&
&\mathstrut+\mathstrut
&\displaystyle\frac12\frac{\partial^2 f}{\partial y^2}(x_0,y_0)&\cdot&(y-y_0)^2
&\mathstrut+\mathstrut
&\displaystyle\frac12\frac{\partial^3 f}{\partial y^2\,\partial x}(x_0,y_0)&\cdot&(x-x_0)(y-y_0)^2
&\mathstrut+\mathstrut
&&&\dots
\\[-5pt]
&
&\mathstrut+\mathstrut
&&\vdots&
&\mathstrut+\mathstrut
&&\vdots&
&\mathstrut+\mathstrut
&&&\ddots
\end{array}
\label{buch:fuvar:hessesche:taylor:2d}
\end{equation}
Die rote Spalte ist die Taylor-Reihe $\mathscr{T}_{y_0} f(x_0,y)$ der
partiellen Funktion $y\mapsto f(x_0,y)$ an der Stelle $y_0$.
Die blaue Spalte ist die Taylor-Reihe
\[
\mathscr{T}_{y_0} \frac{\partial f}{\partial x}(x_0,y)
\]
der partiellen Funktion der partiellen ersten Ableitung von $f$ nach $x$
an der Stelle $y_0$.
Die gleiche Konstruktion ist auch ausgehend von der zweiten Entwicklung
\eqref{buch:fuvar:hessesche:taylor:Tfy} möglich und ergibt eine
analoge Summe mit vertauschter Ableitungsreihenfolge.
Für eine beliebig oft stetig differenzierbare Funktion sind die
Ableitungen nach dem Satz von Schwarz nicht abhängig von der Reihenfolge,
es entsteht also die gleiche Reihenentwicklung.

Die Reihenentwicklung \eqref{buch:fuvar:hessesche:taylor:2d} kann auch
\begin{align}
f(x,y)
=
\mathscr{T}_{(x_0,y_0)}f(x,y)
&=
\sum_{k=0}^\infty
\sum_{l=0}^k
\frac{1}{l!\,(k-l)!}
\frac{\partial^k f}{\partial x^l\,\partial y^{k-l}}(x_0,y_0)
(x-x_0)^l(y-y_0)^{k-l}
\notag
\\
&=
\sum_{k=0}^\infty
\frac{1}{k!}
\sum_{l=0}^k
\binom{k}{l}
\frac{\partial^k f}{\partial x^l\,\partial y^{k-l}}(x_0,y_0)
(x-x_0)^l(y-y_0)^{k-l}
\label{buch:fuvar:hessesche:taylor2:summe}
\end{align}
geschrieben werden.

%
%
%
\subsubsection{Multiindizes}
Die Terme der Taylor-Reihe \eqref{buch:fuvar:hessesche:taylor2:summe}
sind charakterisiert durch die beiden Zahlen $l$ und $k-l$, eine symmetrischere
Schreibweise wäre daher
\[
\mathscr{T}_{(x_0,y_0)}f(x,y)
=
\sum_{k=0}^\infty
\frac{1}{k!}
\sum_{l_1+l_2=k}
\frac{k!}{l_1!\,l_2!}
\frac{\partial^k f}{\partial x^{l_1}\,\partial y^{l_2}}(x_0,y_0)
(x-x_0)^{l_1}(y-y_0)^{l_2}.
\]
Eine Taylor-Reihe für Funktionen von $n$ Variablen $x_1,\dots,x_n$ wird
daher durch $n$ Zahlen $l_1,\dots,l_n$ charakterisiert, die wir auch
als $n$-Tupel oder als Vektor schreiben können.

\begin{definition}[Multiindex]
Ein {\em $n$-dimensionaler Multiindex} ist ein $n$-Tupel
$\mathbf{l}\in\mathbb{N}^n$ mit den Komponenten $l_1,\dots,l_n$.
\end{definition}

Damit die Ausdrücke in der Taylor-Entwicklung kompakt geschrieben werden 
können, müssen geeignete Operationen mit Multiindizes formuliert werden.

\begin{definition}[Operationen mit Multiindizes]
Seine $\mathbf{l}$ und $\mathbf{m}$ beides $n$-dimensionale Multiindizes.
Dann sei
\begin{enumerate}
\item {\em Ordnung} eines Multiindex: $|l|=l_1+\dots+l_n$.
\item {\em Summe}: $\mathbf{l}+\mathbf{m}$ ist der Multiindex mit
den Komponenten $l_1+m_1,\dots,l_n+m_n$.
\item {\em Fakultät}: $\mathbf{l}! = l_1!\, \dots\, l_n!$.
\item {\em Multinomialkoeffizient}: $\displaystyle
\binom{|\mathbf{l}|}{\mathbf{l}}
=
\frac{|\mathbf{l}|!}{\mathbf{l}!}
=
\frac{(l_1+\dots+l_n)!}{l_1!\,\dots\,l_n!}
$
\item {\em Potenzen}: Sei $x\in\mathbb{R}^n$ ein $n$-dimensionaler Vektor,
dann ist $x^{\mathbf{l}} = x_1^{l_1}\cdot\ldots\cdot x_n^{l_n}$.
\item {\em Partielle Ableitungen}: Sei $f\colon\mathbb{R}^n\to\mathbb{R}$
eine $|\mathbf{l}|$-mal stetig differenzierbare Funktion, dann sind die
partiellen Ableitungen
\[
\frac{\partial^{\mathbf{l}} f}{\partial x^{\mathbf{l}}}(x)
=
\frac{\partial^{l_1+\dots+l_n} f}{\partial x_1^{l_1}\,\cdots\,\partial x_n^{l_n}}(x).
\]
\end{enumerate}
\end{definition}
Die Multindex-Notation ist konsistent mit üblichen algebraischen
Rechenregeln, zum Beispiel gilt
$x^{\mathbf{l}}x^{\mathbf{m}}=x^{\mathbf{l}+\mathbf{m}}$
oder 
\[
\frac{\partial^{\mathbf{l}}}{\partial x^{\mathbf{l}}}
\frac{\partial^{\mathbf{m}}f}{\partial x^{\mathbf{m}}}(x)
=
\frac{\partial^{\mathbf{l}+\mathbf{m}}f}{\partial x^{\mathbf{l}+\mathbf{m}}}(x).
\]
Die Multinomialformel für die $k$-te Potenz der Summe
$x_1+\dots+x_n$ der Komponenten des Vektors $x\in\mathbb{R}^n$
kann damit ebenfalls sehr kompakt als
\[
(x_1+\dots+x_n)^{k}
=
\sum_{|\mathbf{l}|=k}
\binom{k}{\mathbf{l}} x^{\mathbf{l}}
\]
geschrieben werden.

%
% Taylor-Reihe von Funktionen R^n -> R
%
\subsubsection{Talyor-Reihe für Funktionen von $n$ Variablen}
Mit der Multiindex-Notation lässt sich jetzt auch die Taylor-Reihe
für eine Funktion $f\colon\mathbb{R}^n\to\mathbb{R}$ von $n$ Variablen
aufschreiben.
Dazu schreiben wir zunächst die Taylor-Reihe
\eqref{buch:fuvar:hessesche:taylor2:summe}
einer Funktion von zwei Variablen in Multiindex-Notation als
\[
\mathscr{T}_{(x_0,y_0)}f(x,y)
=
\sum_{k=0}^\infty
\frac{1}{k!}
\sum_{|\mathbf{l}|=k}
\frac{k!}{\mathbf{l}!}
\frac{\partial^k f}{\partial x^{\mathbf{l}}}
(x_0,y_0)
\begin{pmatrix}
x-x_0\\
y-y_0
\end{pmatrix}^{\mathbf{l}}
=
\sum_{k=0}^\infty
\frac{1}{k!}
\sum_{|\mathbf{l}|=k}
\binom{k}{\mathbf{l}}
\frac{\partial^{\mathbf{l}} f}{\partial x^{\mathbf{l}}}(x_0,y_0)
\begin{pmatrix}
x-x_0\\
y-y_0
\end{pmatrix}^{\mathbf{l}}.
\]
In Analogie dazu können wir jetzt auch die Taylor-Reihe einer Funktion
von $n$-Variablen schreiben.

\begin{definition}[Taylor-Reihe]
Sei $f\colon\mathbb{R}^n\to\mathbb{R}$ eine beliebig oft stetig
differenzierbare Funktion.
Die {\em Taylor-Reihe} von $f$ im Punkt $x_*$ ist die Reihe
\[
\mathscr{T}_{x_*} f(x)
=
\sum_{k=0}^\infty
\frac{1}{k!}
\sum_{|\mathbf{l}|=k}
\binom{k}{\mathbf{l}}
\frac{\partial^k f}{\partial x^{\mathbf{l}}}(x_*) (x-x_*)^{\mathbf{l}}
=
\sum_{k=0}^\infty
\sum_{|\mathbf{l}|=k}
\frac{1}{\mathbf{l}!}
\frac{\partial^k f}{\partial x^{\mathbf{l}}}(x_*) (x-x_*)^{\mathbf{l}}.
\]
\end{definition}

Für viele praktisch wichtige Funktionen ist die Taylor-Reihe konvergent
und kann damit zur Beurteilung verwendet werden, ob ein kritischer Punkt
$x_*\in\mathbb{R}^n$ ein Extremum ist.
Wie im Fall einer Variablen ist ein kritischer Punkt $x_*$ kein Extremum,
wenn die niedrigste Ordnung der in $x_*$ nicht verschwindenden partiellen
Ableitungen ungerade ist.
Für gerade Ableitungen ist es deutlich schwieriger, notwendige Kriterien
für ein Extremum zu formulieren.
Die Funktion
\[
f(x,y)
=
x^4 + y^5
\]
hat $4$ als niedrigste Ordnung einer nicht verschwindenden partiellen
Ableitung in $0$.
Trotzdem ist der Nullpunkt kein Minimum, denn die partielle Funktion
$y\mapsto f(0,y)=y^5$ einen Sattelpunkt bei $y=0$ hat.
Es würde aber zum Bespiel genügen, wenn
\[
\sum_{2r=|\mathbf{l}|}
\frac{\partial^{2r}f}{\partial \mathbf{l}}
(x-x_*)^{x^{\mathbf{l}}}
>
0
\]
ist für alle Vektoren $x\in\mathbb{R}\setminus\{x_*\}$.
Für $r=1$ ist dies das Kriterium von
Satz~\ref{buch:fuvar:hessesche:satz:kriterium}.



